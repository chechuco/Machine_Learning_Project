data <- data.frame(x, CompressiveStrength, Age, FlyAsh)
x= seq(1,length(concrete$CompressiveStrength))
data <- data.frame(x, CompressiveStrength, Age, FlyAsh)
CompressiveStrength = concrete$CompressiveStrength
data <- data.frame(x, CompressiveStrength, Age, FlyAsh)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength), colour = cutAge)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength), colour = as.factor(cutAge))
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength), colour = Age)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength), colour = FlyAsh)
cutFlyAsh <- cut2(concrete$FlyAsh, g=6)
length(cutFlyAsh)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength), colour = cutFlyAsh)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutFlyAsh))
rm(list =ls())
data(concrete)
library(caret); library(Hmisc)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
#Preparar index
x= seq(1,length(concrete$CompressiveStrength))
CompressiveStrength = concrete$CompressiveStrength
data <- data.frame(x, CompressiveStrength)
#Plot
cutAge <- cut2(Age, g=6)
cutFlyAsh <- cut2(concrete$FlyAsh, g=6)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutAge))
cutAge <- cut2(Age, g=6)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutAge))
cutAge <- cut2(Age, g=6)
cutAge <- cut2(concrete$Age, g=6)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutAge))
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutFlyAsh))
cutFlyAsh <- cut2(concrete$FlyAsh, g=4)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutFlyAsh))
cutCement <- cut2(concrete$Cement, g=4)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutCement))
cutBlastFurnaceSlag <- cut2(concrete$BlastFurnaceSlag, g=4)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutBlastFurnaceSlag))
cutSuperplasticizer <- cut2(concrete$Superplasticizer, g=4)
cutCoarseAggregater <- cut2(concrete$CoarseAggregate, g=4)
cutFineAggregate <- cut2(concrete$FineAggregate, g=4)
cutAge <- cut2(concrete$Age, g=4)
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutSuperplasticizer))
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutCoarseAggregater))
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutFineAggregate))
ggplot(data) + geom_point(aes(x=x,y=CompressiveStrength, colour = cutAge))
rm(list=ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Selecciona las columnas deseadas
myTraining <- training[, grep("diagnosis|^IL", names(training))]
myTesting <- training[, grep("diagnosis|^IL", names(testing))]
# Ajustamos el modelo
ctrl <- trainControl(preProcOptions = list(pcaComp = 5)) #list(thresh = 0.8)
modelFit <- train(myTraining$diagnosis ~ .,method="glm",preProcess="pca",trControl = ctrl, data=myTraining)
confusionMatrix(myTesting$diagnosis,predict(modelFit,myTesting))
ctrl_1 <- trainControl(preProcOptions = list(pcaComp = 5))
ctrl_2 <- trainControl(preProcOptions = list(thresh = 0.8))
modelFit1 <- train(myTraining$diagnosis ~ .,method="glm",preProcess="pca",trControl = ctrl_1, data=myTraining)
modelFit1
modelFit2 <- train(myTraining$diagnosis ~ .,method="glm",preProcess="pca",trControl = ctrl_2, data=myTraining)
confusionMatrix(myTesting$diagnosis,predict(modelFit2,myTesting))
rm(list= ls())
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Selecciona las columnas deseadas
myTraining <- training[, grep("diagnosis|^IL", names(training))]
myTesting <- training[, grep("diagnosis|^IL", names(testing))]
#Modelo no PCA
modelFit2a <- train(diagnosis ~ ., method="glm", data=training)
predic2a <- predict(modelFit2a, newdata = testing)
confusionMatrix(predic2a, testing$diagnosis)
ctrl_2 <- trainControl(preProcOptions = list(thresh = 0.8))
modelFit2b <- train(diagnosis ~ .,method="glm",preProcess="pca",trControl = ctrl_2, data=myTraining)
predic2b <- predict(modelFit2b, newdata = testing)
confusionMatrix(predic2b, testing$diagnosis)
rm(list= ls())
par(mar=c(0,0,0,0)); set.seed(1234); x = rep(1:4,each=4); y = rep(1:4,4)
plot(x,y,xaxt="n",yaxt="n",cex=3,col=c(rep("blue",15),rep("red",1)),pch=19)
par(mar=c(0,0,0,0));
plot(x,y,xaxt="n",yaxt="n",cex=3,col=c(rep("blue",8),rep("red",8)),pch=19)
par(mar=c(0,0,0,0));
plot(x,y,xaxt="n",yaxt="n",cex=3,col=c(rep("blue",8),rep("red",8)),pch=19)
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <- createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <- iris[inTrain,]
testing <- iris[-inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <- train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
library(rattle)
fancyRpartPlot(modFit$finalModel)
install.packages("ratle")
install.packages("rattle")
library(rattle)
packageStatus()
packageVersion()
packageVersion(caret)
packageVersion("caret")
packageVersion("ElemStatLearn")
packageVersion("pg00")
packageVersion("pgmm")
install.packages("pgmm")
packageVersion("pgmm")
packageVersion("rpart")
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
inTrain = createDataPartition(segmentationOriginal$Case, p = 0.60)[[1]]
training = segmentationOriginal[ inTrain,]
testing = segmentationOriginal[-inTrain,]
modelFit <- train(type ~.,data=training, method="rpart")
modelFit <- train(Case ~.,data=training, method="rpart")
predictions <- predict(modelFit,newdata=testing)
predictions
confusionMatrix(predictions,testing$type)
confusionMatrix(predictions,testing$Case)
mydata <- data.frame(TotalIntench2=c(23, 50, 57))
mydata <- data.frame(TotalIntench2=c(23, 50, 57), FiberWidthCh1=c(10, 10, 8), PerimStatusCh1=c(1, NA(), NA()), VarIntenCh4=c(NA(), 100, 100))
mydata <- data.frame(TotalIntench2=c(23, 50, 57), FiberWidthCh1=c(10, 10, 8), PerimStatusCh1=c(1, NA, NA, VarIntenCh4=c(NA, 100, 100))
mydata <- data.frame(TotalIntench2=c(23, 50, 57), FiberWidthCh1=c(10, 10, 8), PerimStatusCh1=c(1, NA, NA), VarIntenCh4=c(NA, 100, 100))
predictions <- predict(modelFit,newdata=mydata)
predictions
mydata <- data.frame(TotalIntench2=c(23), FiberWidthCh1=c(10), PerimStatusCh1=c(1))
predictions <- predict(modelFit,newdata=mydata)
predictions
rm(list = ls())
library(pgmm)
data(olive)
olive = olive[,-1]
model = train(Area ~ ., method = 'rpart', data = olive)
fancyRpartPlot(model$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(model, newdata)
rm(list = ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
model = train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, method = 'glm', family = 'binomial', data = trainSA)
trainPred = predict(model, trainSA)
testPred = predict(model, testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
trainMissClass = missClass(trainSA$chd, trainPred)
testMissClass = missClass(testSA$chd, testPred)
rm(list = ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y = as.factor(vowel.train$y)
vowel.test$y = as.factor(vowel.test$y)
set.seed(33833)
model = train(y ~ ., method = 'rf', data = vowel.train, prox = TRUE)  # This takes some time...
print(model)
vi = varImp(model$finalModel)
vi = data.frame(var = 1:nrow(vi), imp = vi$Overall)
vi[order(vi$imp),]
getwd()
setwd("/Course Project_Machine Learning")
setwd("Course Project_Machine Learning")
getwd()
rm(list=ls())
tr_data <- read.csv("pml-training.csv",header = TRUE, na.strings = "", stringsAsFactors = FALSE)
table(tr_data)
table(tr_data$carlitos)
table(tr_data$user_name)
table(tr_data(user_name, num_window)
)
table(tr_data[user_name, num_window])
table(tr_data$user_name, tr_data$num_window])
table(tr_data$user_name, tr_data$num_window)
table(tr_data$num_window)
table(tr_data$new_window)
tr_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
tr_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
rm(tr_data)
train_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
test_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "./pml-training.csv")
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "./pml-testing.csv")
train_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
test_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
train_data$classe
borrar <- train_data[-c(1:7),]
borrar <- train_data[, -c(1:7)]
rd_train_data <- train_data[, -c(1:7)]
rd_test_data <- test_data[, -c(1:7)]
rm(borrar)
summary(train_data)
nzv <- nearZeroVar(pml.training)
nzv <- nearZeroVar(rd_train_data)
head(nzv)
dim(train_data)
View(rd_test_data)
rm(rd_test_data)
library("caret")
?nearZeroVar
nzv <- nearZeroVar(rd_train_data)
rd_train_data <- rd_train_data[, -nzv]
dim(rd_train_data)
?DMwR
??DMwR
training.dena <- rd_train_datag[ , colSums(is.na(rd_train_data)) == 0]
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) == 0]
table(rd_train_data$skewness_yaw_arm)
table(rd_train_data$skewness_roll_belt.1)
colSums(is.na(rd_train_data)) == 0
head(rd_train_data$skewness_roll_belt.1)
summary(rd_train_data$skewness_roll_belt.1)
colSums(is.na(rd_train_data)) < 400
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < 400]
dim(training.dena)
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < 4000]
dim(training.dena)
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < 40000]
dim(training.dena)
dim(training.dena[1])
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < 0.25 * dim(training.dena[1])]
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < (0.25 * dim(training.dena[1]))]
0.25 * dim(training.dena[1])
dim(training.dena[1])
dim(training.dena[1])[1]
dim(training.dena[1])[1]*0.25
dim(training.dena)[1]*0.25
dim(rd_train_data)[1]*0.25
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < 0.25 * dim(training.dena[1])]
dim(training.dena)
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < (0.25 * dim(training.dena[1]))]
dim(training.dena)
dim(rd_train_data)[1]*0.25
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < dim(rd_train_data)[1]*0.25]
training.dena <- rd_train_data[ , colSums(is.na(rd_train_data)) < dim(rd_train_data)[1]*0.25]
dim(training.dena)
train_data_irr <- train_data[, -c(1:7)]
rm(list=ls())
library("caret")
#Get the data
if (!file.exists("./pml-training.csv")) {
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
destfile = "./pml-training.csv")
}
if (!file.exists("./pml-testing.csv")) {
download.file("http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
destfile = "./pml-testing.csv")
}
#Load training set
train_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
#Load test set
test_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
```
# Exploratory data analysis
Both datasets have a large number of variables:
```{r}
dim(train_data)
dim(test_data)
```
First thing, we must reduce the number of variables in the train set, to simplify the modeling step and reduce the process time:
1.-Irrelevant variables: we can disregard some of the variables becouse they are irrelevant for our study, namely the first seven of them: number of row, user_name, raw_timestamp_part1, raw_timestamp_part2, cvtd_timestamp, new_window and num_window:
```{r}
#Cleaning data: irrelevant variables
train_data_irr <- train_data[, -c(1:7)]
train_data_nav <- train_data_irr[ , colSums(is.na(train_data_irr)) < dim(train_data_irr)[1]*0.25]
dim(train_data_nav)
lowVar <- nearZeroVar(train_data_nav)
train_data_nzv <- train_data_nav[, -lowVar]
dim(train_data_nzv)
train_data_nzv
lowVar
lowVar <- nearZeroVar(train_data_irr)
train_data_nzv <- train_data_irr[, -lowVar]
dim(train_data_nzv)
lowVar <- nearZeroVar(train_data_nav)
train_data_nzv <- train_data_nav[, -lowVar]
dim(train_data_nzv)
#Cleaning data: >25%NA variables
train_data_nav <- train_data_irr[ , colSums(is.na(train_data_irr)) < dim(train_data_irr)[1]*0.25]
dim(train_data_nav)
train_data_nav <- train_data_irr[ , colSums(is.na(train_data_irr)) < dim(train_data_irr)[1]*0.2]
dim(train_data_nav)
lowVar <- nearZeroVar(train_data_nav)
train_data_nzv <- train_data_nav[, -lowVar]
dim(train_data_nzv)
View(train_data)
View(train_data)
table(train_data$classe)
column(train_data$classe)
col(train_data$classe)
col(train_data)
ncol(train_data$classe)
head(train_data$classe)
?row
?head
head(train_data, n=1)
colSums(is.na(train_data_irr)
)
preProcData <- function(rowData, Unrev){
dataset <- rowData[, -Unrev]
dataset
}
preProcData(train_data, c(1:7))
preProcData <- function(rowData, Unrev){
dataset <- rowData[, -Unrev]
invisible(dataset)
}
preProcData(train_data, c(1:7))
datasets
dataset
preProcData <- function(rowData, Unrev){
preProcDataset <- rowData[, -Unrev]
invisible(preProcDataset)
}
borrar <- preProcData(train_data, c(1:7))
preProc <- function(rowData, Unrev){
# Irrelevant variables: disregard columns indicated in the parameter Unrev
dataset_irrev <- rowData[, -Unrev]
#Cleaning data: near-zero variance variables
lowVar <- nearZeroVar(dataset_irrev)
dataset_nzv <- dataset_irrev[, -lowVar]
#Cleaning data: >25%NA variables
dataset_nav <- dataset_nzv[ , colSums(is.na(dataset_nzv)) < dim(dataset_nzv)[1]*0.2]
invisible(dataset_nav)
}
borrar <- preProcData(train_data, c(1:7))
borrar <- preProcData(train_data, c(1:7))
rm(list=ls())
train_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
#Load test set
test_data <- read.csv("pml-testing.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
# Check dimensions
dim(test_data)
dim(test_data)
dim(train_data)
dim(test_data)
preProc <- function(rowData, Unrev){
# Irrelevant variables: disregard columns indicated in the parameter Unrev
dataset_irrev <- rowData[, -Unrev]
#Cleaning data: near-zero variance variables
lowVar <- nearZeroVar(dataset_irrev)
dataset_nzv <- dataset_irrev[, -lowVar]
#Cleaning data: >25%NA variables
dataset_nav <- dataset_nzv[ , colSums(is.na(dataset_nzv)) < dim(dataset_nzv)[1]*0.2]
invisible(dataset_nav)
}
borrar <- preProcData(train_data, c(1:7))
borrar <- preProc(train_data, c(1:7))
preProc <- function(rowData, unrev){
# Function that performs several preprocessing tasks. Parameters:
#   · rowData: data frame, the dataset to clean
#   · unrev: vector, numcolumns to disregard from the dataset
# 1:Irrelevant variables: disregard columns indicated in the parameter Unrev
dataset_irrev <- rowData[, -unrev]
# 2: Near-zero variance variables: ariables with just one value or very low variance aren´t helpful for prediction, the function remove them:
lowVar <- nearZeroVar(dataset_irrev)
dataset_nzv <- dataset_irrev[, -lowVar]
# 3: NA variables: disregard variables having more than 80% of missing values
dataset_nav <- dataset_nzv[ , colSums(is.na(dataset_nzv)) < dim(dataset_nzv)[1]*0.2]
invisible(dataset_nav)
dim(dataset_nav)
}
borrar <- preProc(train_data, c(1:7))
dim(dataset_nav)
preProc <- function(rowData, unrev){
# Function that performs several preprocessing tasks. Parameters:
#   · rowData: data frame, the dataset to clean
#   · unrev: vector, numcolumns to disregard from the dataset
# 1:Irrelevant variables: disregard columns indicated in the parameter Unrev
dataset_irrev <- rowData[, -unrev]
# 2: Near-zero variance variables: ariables with just one value or very low variance aren´t helpful for prediction, the function remove them:
lowVar <- nearZeroVar(dataset_irrev)
dataset_nzv <- dataset_irrev[, -lowVar]
# 3: NA variables: disregard variables having more than 80% of missing values
dataset_nav <- dataset_nzv[ , colSums(is.na(dataset_nzv)) < dim(dataset_nzv)[1]*0.2]
invisible(dataset_nav)
}
table(train_data$classe)
train_set <- preProc(train_data, c(1:7))
dim(train_set)
library(randomForest)
ctrl <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
modFit <- train(classe ~ ., data = train.set, method = "rf",
prof = TRUE, trControl = ctrl)
modFit <- train(classe ~ ., data = train_set, method = "rf",
prof = TRUE, trControl = ctrl)
modFit
results <- modFit$results
round(max(results$Accuracy), 4) * 100
?createDataPartition
inTrain <- createDataPartition(y=train_set$classe,p=0.6, list=FALSE)
training <- train_set[inTrain,]
testing <- train_set[-inTrain,]
modFit <- train(classe ~ ., data = training, method = "rf",
prof = TRUE, trControl = ctrl)
modFit
results <- modFit$results
round(max(results$Accuracy), 4) * 100
hist(as.numeric(tr_training$classe), axes = FALSE, xlab = "Exercise Class",
col="red", main = "Histogram of variable 'classe'")
axis(2)
axis(1, at = c(1,2,3,4,5), labels = c("A","B","C","D","E"))
tr_training <- train_set[inTrain,]
tr_testing <- train_set[-inTrain,]
hist(as.numeric(tr_training$classe), axes = FALSE, xlab = "Exercise Class",
col="red", main = "Histogram of variable 'classe'")
axis(2)
axis(1, at = c(1,2,3,4,5), labels = c("A","B","C","D","E"))
hist(as.numeric(tr_training$classe), axes = FALSE, xlab = "Exercise Class",
col="red", main = "Histogram of variable 'classe'")
axis(2)
axis(1, at = c(1,2,3,4,5), labels = c("A","B","C","D","E"))
table(tr_training$classe)
table(tr_testing$classe)
summary(table(tr_testing$classe))
seed(1970)
trct <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
modFit <- train(classe ~ ., data = tr_training, method = "rf",
prof = TRUE, trControl = trct)
modFit
?seed
trCtrl <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
typeof((trCtrl))
class(trCtrl)
errorMeasure <- confusionMatrix(tr_testing$classe, predict(modelFit, tr_testing))
errorMeasure
errorMeasure <- confusionMatrix(tr_testing$classe, predict(modFit, tr_testing))
errorMeasure
rm(list=ls())
train_data <- read.csv("pml-training.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
#Load test set
test_data <- read.csv("pml-testing.csv",header = TRUE, na.strings = c("NA", ""), stringsAsFactors = FALSE)
#Check dimensions
dim(train_data)
dim(test_data)
preProc <- function(rowData, unrev){
# Function that performs several preprocessing tasks. Library caret needs to be loaded.
# Parameters:
#   · rowData: data frame, the dataset to clean
#   · unrev: vector, numcolumns to disregard from the dataset
# 1:Irrelevant variables: disregard columns indicated in the parameter Unrev
dataset_irrev <- rowData[, -unrev]
# 2: Near-zero variance variables: ariables with just one value or very low variance aren´t helpful for prediction, the function remove them:
lowVar <- nearZeroVar(dataset_irrev)
dataset_nzv <- dataset_irrev[, -lowVar]
# 3: NA variables: disregard variables having more than 80% of missing values
dataset_nav <- dataset_nzv[ , colSums(is.na(dataset_nzv)) < dim(dataset_nzv)[1]*0.2]
invisible(dataset_nav)
}
train_set <- preProc(train_data, c(1:7))
dim(train_set)
inTrain <- createDataPartition(y=train_set$classe,p=0.6, list=FALSE)
tr_training <- train_set[inTrain,]
tr_testing <- train_set[-inTrain,]
set.seed(1970)
trCtrl <- trainControl(method = "cv", number = 4, allowParallel = TRUE)
modFit <- train(classe ~ ., data = tr_training, method = "rf",
prof = TRUE, trControl = trCtrl)
modFit
results <- modFit$results
round(max(results$Accuracy), 4) * 100
?rfcv
fitResults <- modFit$results
round(max(fitResults$Accuracy), 4) * 100
confM <- confusionMatrix(tr_testing$classe, predict(modFit, tr_testing))
confM
oosError <- 1-confM$overall[1]
oosError
oosError <- round(1-confM$overall[1], 4) * 100
oosError
oosError <- round(1-as.vector(confM$overall[1]), 4) * 100
oosError
oosError <- round(1-as.vector(confM$overall[1]), 4) * 100
oosError
submit <- predict(modFit, test_data)
submit
varImpPlot(modFit)
table(train_data$user_name)
